\thispagestyle{plain}

\textbf{Abstract}

\vspace{1cm}


In recent years, wherever you may look, you will come across a machine learning model. It significantly becomes an integral part of various industries revolving all parts of life.
The catch is - machine learning models became much more complex and less intuitive to understand.

State-of-the-art models contain billions of parameters. A lot of real word integration of these models face major trust issues – how can one know that the model performs in the right way? How can we ensure that the model, for example, classify based of the features that ‘really’ matter and not on some bias within the data? (Amazon, for example, shut down a risk prediction model that is discovered to be biased towards woman).
Explainable AI (xAI) is a relatively new area that try to answer exactly these questions, and to extract insights that are understandable to humans.

In this project, I have implemented a few xAI methods for GAN – machine learning framework that produce generative model. It was trained for the task of handwriting generation. I will try to examine (qualitatively) the kind of results from each one of the explainability methods by using a dummy dataset (MNIST). Next, I will explore these methods on a ScrabbleGAN model, which is able to generate handwritten text in various lengths and styles. 



